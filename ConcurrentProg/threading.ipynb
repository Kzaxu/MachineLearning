{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f0146373c32dc03127ec6bba3fe790e42078c8d1cee5cdf638507b26396ff2a1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## python 创建多线程的方法\n",
    "1. 准备一个函数 my_func\n",
    "2. 创建一个线程\n",
    "3. 启动线程\n",
    "4. 等待结束"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading \n",
    "\n",
    "def my_func(a, b):\n",
    "    return a + b \n",
    "\n",
    "t = threading.Thread(target=my_func, args=(1,2))\n",
    "t.start()\n",
    "t.join()"
   ]
  },
  {
   "source": [
    "**例子**  \n",
    "爬取cnblogs数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blog_spider\n",
    "\n",
    "def single_thread():\n",
    "    print(\"single thread begin\")\n",
    "    for url in blog_spider.urls:\n",
    "        blog_spider.craw(url)\n",
    "    print(\"single thread end\")\n",
    "\n",
    "\n",
    "def multi_thread():\n",
    "    print(\"multi thread begin\")\n",
    "    threads = []\n",
    "    for url in blog_spider.urls:\n",
    "        threads.append(\n",
    "            threading.Thread(target=blog_spider.craw, args=(url,))\n",
    "        )\n",
    "\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    print(\"multi thread end\")"
   ]
  },
  {
   "source": [
    "%%capture out1\n",
    "import time \n",
    "\n",
    "bg = time.time()\n",
    "single_thread()\n",
    "ed = time.time()\n",
    "print(f\"single thread cost: {round(ed-bg,3)}s\")\n",
    "\n",
    "bg = time.time()\n",
    "multi_thread()\n",
    "ed = time.time()\n",
    "print(f\"multi thread cost: {round(ed-bg,3)}s\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 8,
   "outputs": []
  },
  {
   "source": [
    "## 多组件pipeline技术架构 \n",
    "生产者-消费者  \n",
    "此时使用多线程数据通信的queue.Queue"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "craw1 do_crawparse0craw0 parse do_craw url_queue.size=47\n",
      "  url_queue.size=47html_queue.size=0\n",
      "\n",
      "parse1 parse html_queue.size=0\n",
      "craw2 do_craw url_queue.size=47\n",
      "parse0 parse html_queue.size=0\n",
      "craw1 do_craw url_queue.size=45\n",
      "craw2 do_craw url_queue.size=45\n",
      "parse1 parse html_queue.size=1\n",
      "parse0 parse html_queue.size=0\n",
      "craw0 do_craw url_queue.size=44\n",
      "parse1 parse html_queue.size=0\n",
      "craw1 do_craw url_queue.size=42\n",
      "parse0 parse html_queue.size=0\n",
      "craw2 do_craw url_queue.size=42\n",
      "craw0 do_craw url_queue.size=41\n",
      "parse0 parse html_queue.size=1\n",
      "craw1 do_craw url_queue.size=40\n",
      "parse1 parse html_queue.size=1\n",
      "parse0 parse html_queue.size=0\n",
      "craw2 do_craw url_queue.size=39\n",
      "craw1 do_craw url_queue.size=38\n",
      "craw0 do_craw url_queue.size=37\n",
      "parse0 parse html_queue.size=2\n",
      "craw2 do_craw url_queue.size=36\n",
      "parse1 parse html_queue.size=2\n",
      "craw1 do_craw url_queue.size=35\n",
      "craw0parse0 parse html_queue.size=3\n",
      " do_craw url_queue.size=34\n",
      "craw2 do_craw url_queue.size=33\n",
      "parse1 parse html_queue.size=3\n",
      "parse0 parse html_queue.size=2\n",
      "craw0 do_craw url_queue.size=30\n",
      "craw1 do_craw url_queue.size=30\n",
      "craw2 do_craw url_queue.size=30\n",
      "parse1 parse html_queue.size=4\n",
      "parse0 parse html_queue.size=3\n",
      "parse1 parse html_queue.size=2\n",
      "craw1 do_craw url_queue.size=27\n",
      "craw0 do_craw url_queue.size=27\n",
      "craw2 do_craw url_queue.size=27\n",
      "parse1 parse html_queue.size=4\n",
      "parse0 parse html_queue.size=3\n",
      "parse1 parse html_queue.size=2\n",
      "craw0 do_craw url_queue.size=24\n",
      "craw1 do_craw url_queue.size=24\n",
      "craw2 do_craw url_queue.size=24\n",
      "parse0 parse html_queue.size=4\n",
      "craw1 do_craw url_queue.size=22\n",
      "craw0 do_craw url_queue.size=22\n",
      "parse1 parse html_queue.size=5\n",
      "craw2 do_craw url_queue.size=19\n",
      "craw0craw1 do_craw url_queue.size=19\n",
      " do_craw url_queue.size=19\n",
      "parse1 parse html_queue.size=6\n",
      "parse0 parse html_queue.size=6\n",
      "craw2 do_craw url_queue.size=17\n",
      "craw0 do_craw url_queue.size=17\n",
      "parse0 parse html_queue.size=7\n",
      "craw1 do_craw url_queue.size=16\n",
      "craw0 do_craw url_queue.size=15\n",
      "parse1 parse html_queue.size=8\n",
      "craw2 do_craw url_queue.size=14\n",
      "craw0 do_craw url_queue.size=13\n",
      "parse0 parse html_queue.size=9\n",
      "craw1 do_craw url_queue.size=11\n",
      "craw2 do_craw url_queue.size=10\n",
      "craw0 do_craw url_queue.size=10\n",
      "parse1 parse html_queue.size=11\n",
      "craw2 do_craw url_queue.size=8\n",
      "craw0 do_craw url_queue.size=8\n",
      "parse1 parse html_queue.size=12\n",
      "parse0 parse html_queue.size=11\n",
      "craw1 do_craw url_queue.size=7\n",
      "craw2 do_craw url_queue.size=6\n",
      "craw0 do_craw url_queue.size=5\n",
      "parse1 parse html_queue.size=13\n",
      "parse0 parse html_queue.size=12\n",
      "craw1 do_craw url_queue.size=4\n",
      "craw2 do_craw url_queue.size=2\n",
      "craw0 do_craw url_queue.size=2\n",
      "parse0parse1 parse html_queue.size=13\n",
      " parse html_queue.size=13\n",
      "craw1 do_craw url_queue.size=1\n",
      "craw2 do_craw url_queue.size=0\n",
      "parse1 parse html_queue.size=14\n",
      "parse0 parse html_queue.size=13\n",
      "parse1 parse html_queue.size=12\n",
      "parse0 parse html_queue.size=11\n",
      "parse1 parse html_queue.size=10\n",
      "parse0parse1 parse html_queue.size=8\n",
      " parse html_queue.size=8\n",
      "parse0 parse html_queue.size=7\n",
      "parse1 parse html_queue.size=6\n",
      "parse1 parse html_queue.size=5\n",
      "parse0 parse html_queue.size=4\n",
      "parse0 parse html_queue.size=3\n",
      "parse1 parse html_queue.size=2\n",
      "parse0 parse html_queue.size=1\n",
      "parse0 parse html_queue.size=0\n"
     ]
    }
   ],
   "source": [
    "%%capture out2\n",
    "\n",
    "import queue\n",
    "import blog_spider\n",
    "import time \n",
    "import random\n",
    "import threading \n",
    "\n",
    "def do_craw(url_queue:queue.Queue, html_queue:queue.Queue):\n",
    "    while True:\n",
    "        try:\n",
    "            url = url_queue.get(timeout=10)\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        html = blog_spider.craw_text(url)\n",
    "        html_queue.put(html)\n",
    "        print(threading.current_thread().name, \"do_craw\", f\"url_queue.size={url_queue.qsize()}\")\n",
    "        time.sleep(random.randint(1, 2))\n",
    "\n",
    "\n",
    "def do_parse(html_queue, fout):\n",
    "    while True:\n",
    "        try:\n",
    "            html = html_queue.get(timeout=10)\n",
    "        except queue.Empty:\n",
    "            break\n",
    "        results = blog_spider.parse(html)\n",
    "        for result in results:\n",
    "            fout.write(str(result)+\"\\n\")\n",
    "        print(threading.current_thread().name, \"parse\", f\"html_queue.size={html_queue.qsize()}\")\n",
    "        time.sleep(random.randint(1, 2))\n",
    "\n",
    "url_queue = queue.Queue()\n",
    "html_queue = queue.Queue()\n",
    "for url in blog_spider.urls:\n",
    "    url_queue.put(url)\n",
    "\n",
    "threads = []\n",
    "\n",
    "for i in range(3):\n",
    "    t = threading.Thread(target=do_craw, args=(url_queue, html_queue), name=f\"craw{i}\")\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "fout = open(\"./data/1.txt\", \"w\")\n",
    "for i in range(2):\n",
    "    t = threading.Thread(target=do_parse, args=(html_queue, fout), name=f\"parse{i}\")\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "fout.close()"
   ]
  },
  {
   "source": [
    "## 线程安全\n",
    "\n",
    "#### 概念\n",
    "线程安全指某个函数、函数库在多线程环境中被调用时，能够正确地处理多个线程之间的共享变量，事程序功能正确完成  \n",
    "\n",
    "由于线程的执行随时会发生切换，就造成了不可预料的结果，出现线程不安全  \n",
    "\n",
    "#### Lock 用于解决线程安全问题\n",
    "用法一：\n",
    "```python\n",
    "import threading\n",
    "\n",
    "lock = threading.Lock()\n",
    "lock.acquire()\n",
    "try:\n",
    "    do something\n",
    "finally:\n",
    "    lock.release()\n",
    "```\n",
    "\n",
    "用法二：\n",
    "```python\n",
    "lock = threading.lock()\n",
    "\n",
    "with lock:\n",
    "    do something\n",
    "```\n",
    "\n",
    "hint: 线程安全问题一般出现在if分支内，可以将整个条件判断代码块进行加锁"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ta 取钱成功\nta 余额：200\ntb 取钱失败，余额不足\n200\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time \n",
    "class Account:\n",
    "    def __init__(self, balance):\n",
    "        self.balance = balance\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "def draw(account, amount):\n",
    "    with lock:\n",
    "        if account.balance >= amount:\n",
    "            time.sleep(0.1)\n",
    "            print(threading.current_thread().name, \n",
    "                \"取钱成功\")\n",
    "            account.balance -= amount\n",
    "            print(threading.current_thread().name, \n",
    "                f\"余额：{account.balance}\")\n",
    "        else:\n",
    "            print(threading.current_thread().name, \n",
    "                \"取钱失败，余额不足\")\n",
    "\n",
    "def test01():\n",
    "    account = Account(1000)\n",
    "    ta = threading.Thread(target=draw, args=(account, 800), name=\"ta\")\n",
    "    tb = threading.Thread(target=draw, args=(account, 800), name=\"tb\")\n",
    "\n",
    "    ta.start()\n",
    "    tb.start()\n",
    "    ta.join()\n",
    "    tb.join()\n",
    "\n",
    "    print(account.balance)\n",
    "\n",
    "test01()"
   ]
  },
  {
   "source": [
    "## 线程池\n",
    "\n",
    "#### 线程池原理\n",
    "线程生命周期：  \n",
    "新建 --> 就绪  \n",
    "　　　　　|  \n",
    "　　　　　获得cpu资源    <-- sleep/io结束  \n",
    "　　　　　|　　　　　　　　　　｜  \n",
    "　　　　　运行 --sleep/io--> 阻塞(失去cpu资源)  \n",
    "　　　　　|  \n",
    "终止 <-- run方法执行玩  \n",
    "- 新建线程系统需要分配资源、终止线程系统需要回收资源\n",
    "- 如果可以重用线程，则可以减去新建/终止的开销\n",
    "\n",
    "#### 线程池好处\n",
    "- 适用场景：适合处理突发性大量请求或需要大量线程完成任务、但实际任务处理时间较短\n",
    "- 防御功能：能有效避免系统因为创建线程过多，而导致系统负荷过大相应变慢等问题\n",
    "- 代码优势：更加简洁\n",
    "\n",
    "\n",
    "#### 线程池使用方式\n",
    "用法一：map函数，map结果和argsLs顺序相对应\n",
    "```python\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "with ThreadPoolExecuter() as pool:\n",
    "    results = pool.map(func, argsLs)\n",
    "\n",
    "    for result in results:\n",
    "    print(result)\n",
    "```\n",
    "\n",
    "用法而：future模式，更强大，如果使用as_completed模式则顺序是不定的\n",
    "```python\n",
    "with ThreadPoolExecuter() as pool:\n",
    "    futures = [pool.submit(func, args)\n",
    "               for args in argsLs]\n",
    "               \n",
    "    for future in futures:\n",
    "        print(future.result())\n",
    "    for future in as_completed(futures):\n",
    "        print(future.result())\n",
    "```\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture out3\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import blog_spider\n",
    "\n",
    "with ThreadPoolExecutor() as pool:\n",
    "    htmls = pool.map(blog_spider.craw_text, blog_spider.urls)\n",
    "    htmls = list(zip(blog_spider.urls, htmls))\n",
    "    for u,h in htmls:\n",
    "        print(u, len(h))\n",
    "\n",
    "print(\"craw over\")\n",
    "\n",
    "with ThreadPoolExecutor() as pool:\n",
    "    futures = {}\n",
    "    for u,h in htmls:\n",
    "        future = pool.submit(blog_spider.parse, h)\n",
    "        futures[u] = future \n",
    "    \n",
    "    for u,future in futures.items():\n",
    "        print(u, future.result())"
   ]
  }
 ]
}